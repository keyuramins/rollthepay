# PostgreSQL Migration Guide for RollThePay

This guide provides step-by-step instructions for migrating RollThePay from Filebrowser CSV data source to PostgreSQL 17 with PgBouncer connection pooling.

## Overview

The migration involves:
1. **Pre-migration setup** - Database schema and configuration
2. **Data migration** - Import CSV data from Filebrowser to PostgreSQL
3. **Application updates** - Replace Filebrowser API with PostgreSQL queries
4. **Post-migration cleanup** - Remove Filebrowser dependencies

## Prerequisites

- âœ… PostgreSQL 17 installed and configured
- âœ… PgBouncer installed and configured
- âœ… Node.js 18+ and npm
- âœ… Access to existing Filebrowser CSV data
- âœ… Admin access to the application

## Phase 1: Pre-Migration Setup

### 1.1 Environment Configuration

Copy the environment template and configure your PostgreSQL connection:

```bash
# Copy environment template
cp env.example .env.local

# Edit .env.local with your actual PostgreSQL details
nano .env.local
```

**Required Environment Variables:**

```bash
# PostgreSQL Direct Connection (for migrations, admin tasks)
POSTGRES_URL=postgres://postgres:your_password@your_host:5432/rollthepay

# PgBouncer Connection (for application queries via port 6432)
PGBOUNCER_URL=postgres://postgres:your_password@your_host:6432/rollthepay

# Application database URL (Next.js will use PgBouncer)
DATABASE_URL=postgres://postgres:your_password@your_host:6432/rollthepay

# Connection pool configuration
PGPOOL_MAX=20
PGPOOL_IDLE_TIMEOUT=30000
PGPOOL_CONNECTION_TIMEOUT=10000

# Admin API authentication (generate a secure random key)
ADMIN_API_KEY=your_secure_random_api_key_here

### 1.2 Install Dependencies

```bash
# Install PostgreSQL client and TypeScript execution tools
npm install
```

### 1.3 Test Database Connection

```bash
# Test PostgreSQL connection
npm run db:test-connection
```

Expected output:
```bash
âœ… Connection successful
{
  "success": true,
  "details": {
    "currentTime": "2024-01-15T10:30:00.000Z",
    "pgVersion": "PostgreSQL 17.0",
    "connectionDuration": "45ms",
    "poolConfig": {
      "max": 20,
      "idleTimeout": 30000,
      "connectionTimeout": 10000
    }
  }
}
```

### 1.4 Setup Database Schema

```bash
# Create database schema, indexes, and materialized views
npm run db:setup
```

Expected output:
```bash
ðŸš€ Starting database setup...
ðŸ”Œ Testing database connection...
âœ… Connected to PostgreSQL at 2024-01-15T10:30:00.000Z
ðŸ“‹ PostgreSQL version: PostgreSQL 17.0
ðŸ“– Reading schema file...
ðŸ—ï¸ Creating database schema...
âœ… Database schema created successfully
ðŸ“‹ Created tables: occupations
ðŸ“‹ Created indexes: 12 indexes
ðŸ“‹ Created materialized views: 2 views
ðŸ“‹ Created triggers: 1 trigger
ðŸ§ª Testing basic operations...
âœ… Test record inserted with ID: 1
âœ… Test record retrieved: Test Occupation
âœ… Test record updated: salary = 60000
âœ… Trigger test: contribution_count = 1
âœ… Test record cleaned up
ðŸ”„ Testing materialized view refresh...
âœ… Materialized views refreshed successfully
ðŸŽ‰ Database setup completed successfully!
```

## Phase 2: Data Migration

### 2.1 Run CSV Migration

```bash
# Migrate all CSV data from Filebrowser to PostgreSQL
npm run db:migrate
```

Expected output:
```bash
ðŸš€ Starting CSV â†’ PostgreSQL migration...
ðŸ“… Started at: 2024-01-15T10:30:00.000Z
ðŸ”Œ Testing database connection...
âœ… Connected to PostgreSQL at 2024-01-15T10:30:00.000Z
ðŸ“ Discovering CSV files from Filebrowser...
ðŸ“‹ Found 3 CSV files: countries.csv, states.csv, occupations.csv
ðŸ“Š Processing countries.csv...
  ðŸ“„ Parsed 195 rows from CSV
  âœ… 195 valid records, 0 skipped
  ðŸ’¾ Inserted 195 records into database
  ðŸ“Š Progress: 100% (195/195 records)
ðŸ“Š Processing states.csv...
  ðŸ“„ Parsed 1,247 rows from CSV
  âœ… 1,247 valid records, 0 skipped
  ðŸ’¾ Inserted 1,247 records into database
  ðŸ“Š Progress: 100% (1,247/1,247 records)
ðŸ“Š Processing occupations.csv...
  ðŸ“„ Parsed 25,000 rows from CSV
  âœ… 24,987 valid records, 13 skipped
  ðŸ’¾ Inserted 24,987 records into database
  ðŸ“Š Progress: 100% (24,987/24,987 records)
ðŸ”„ Refreshing materialized views...
âœ… Materialized views refreshed
ðŸŽ‰ Migration completed successfully!
ðŸ“‹ Migration Report:
  ðŸ“ Files processed: 3
  ðŸ“„ Total rows processed: 26,442
  âœ… Records inserted: 26,429
  â­ï¸  Records skipped: 13
  â±ï¸  Duration: 45 seconds
  ðŸ“Š Average: 587 records/second
ðŸ” Verifying migration...
ðŸ“Š Final database statistics:
  ðŸ¢ Total occupations: 24,987
  ðŸŒ Total countries: 195
  ðŸ›ï¸  Total states: 1,247
  ðŸ™ï¸  Total locations: 3,456
âœ… Migration verification completed!
ðŸš€ Database is ready for use!
```

### 2.2 Validate Migration

```bash
# Validate migrated data integrity
npm run db:validate
```

Expected output:
```bash
ðŸ” Starting database schema validation...
ðŸ“… Started at: 2024-01-15T10:35:00.000Z
ðŸ”Œ Testing database connection...
âœ… Connected to PostgreSQL at 2024-01-15T10:35:00.000Z
ðŸ“‹ PostgreSQL version: PostgreSQL 17.0
ðŸ“‹ Validating table structure...
âœ… Found 55 columns in occupations table
ðŸ” Validating indexes...
âœ… Found 12 indexes on occupations table
ðŸ”’ Validating constraints...
âœ… Found 2 constraints on occupations table
ðŸ“Š Validating materialized views...
âœ… Found 2 materialized views
âš¡ Validating triggers...
âœ… Found 1 triggers
ðŸ“Š Checking data integrity...
âœ… Total records: 26,429
âœ… Required fields check:
  - Records with title: 26,429/26,429
  - Records with slug_url: 26,429/26,429
  - Records with country: 26,429/26,429
âœ… No duplicate records found
âœ… Skills JSONB format check:
  - Records with skills: 18,456/26,429
  - Valid skills arrays: 18,456/26,429
âœ… Sample skills data:
  1. [{"name":"JavaScript","percentage":85},{"name":"React","percentage":72}]
  2. [{"name":"Python","percentage":90},{"name":"Django","percentage":68}]
ðŸŒ Checking geographic distribution...
âœ… Geographic distribution:
  - Countries: 195
  - States: 1,247
  - Locations: 3,456
  - Country-only records: 5,234
  - State-only records: 12,345
  - Location-specific records: 8,850
ðŸ’° Checking salary data quality...
âœ… Salary data quality:
  - Records with avg_annual_salary: 24,987/26,429
  - Records with low_salary: 24,987/26,429
  - Records with high_salary: 24,987/26,429
  - Average salary: 75,432
  - Salary range: 25,000 - 250,000
ðŸ“Š Checking materialized view data...
âœ… Materialized view data:
  - mv_country_stats: 195 countries
  - mv_state_stats: 1,247 states
âš¡ Performance check...
âœ… Sample query performance: 12ms
ðŸ” Checking index usage...
âœ… Index usage statistics:
  - idx_occupations_country: 1,234 scans, 26,429 tuples read
  - idx_occupations_slug: 987 scans, 987 tuples read
ðŸŽ‰ Schema validation completed successfully!
```

## Phase 3: Application Testing

### 3.1 Test Application with PostgreSQL

```bash
# Start the development server
npm run dev
```

Visit your application and test:
- âœ… Home page loads
- âœ… Country pages load (e.g., `/australia`)
- âœ… State pages load (e.g., `/australia/victoria`)
- âœ… Location pages load (e.g., `/australia/victoria/melbourne`)
- âœ… Occupation pages load (e.g., `/australia/software-engineer`)
- âœ… Search functionality works
- âœ… Related occupations display correctly

### 3.2 Test Admin API

```bash
# Test admin API endpoints
curl -X GET "http://localhost:3000/api/admin/occupations" \
  -H "x-api-key: your_admin_api_key"

# Test salary update
curl -X PATCH "http://localhost:3000/api/admin/occupations/1" \
  -H "x-api-key: your_admin_api_key" \
  -H "Content-Type: application/json" \
  -d '{"avg_annual_salary": 95000, "low_salary": 75000, "high_salary": 115000}'
```

## Phase 4: Post-Migration Cleanup

### 4.1 Remove Filebrowser Dependencies

After successful migration and testing:

```bash
# Remove Filebrowser environment variables from .env.local
# Delete these lines:
# FILEBROWSER_BASE_URL=...
# FILEBROWSER_API_KEY=...

# Remove Filebrowser files
rm -rf lib/filebrowser/
rm lib/data/filebrowser-parse.ts
rm scripts/migrate-to-filebrowser.js

# Remove from package.json scripts
# Delete: "migrate-to-filebrowser": "node scripts/migrate-to-filebrowser.js"
```

### 4.2 Update Documentation

Update `README.md` to reflect PostgreSQL as the data source:

```markdown
## Data Source

RollThePay now uses PostgreSQL 17 as the primary data store with PgBouncer connection pooling for high-performance concurrent reads and writes.

### Database Schema

- **Single table design**: `occupations` table with optimized indexes
- **User contributions**: Salary fields are easily editable via Admin API
- **Skills data**: Stored as JSONB for flexibility
- **Geographic hierarchy**: Country â†’ State â†’ Location â†’ Occupation
- **Materialized views**: For fast aggregations and statistics

### Admin API

- **CRUD operations**: Full occupation management
- **Salary updates**: Easy user contribution support
- **Bulk imports**: CSV import for data updates
- **Authentication**: Secure API key-based access
```

## Troubleshooting

### Common Issues

#### 1. Database Connection Failed

**Error:** `Connection failed: ECONNREFUSED`

**Solution:**
```bash
# Check PostgreSQL is running
sudo systemctl status postgresql

# Check connection details in .env.local
npm run db:test-connection

# Verify PostgreSQL is listening on correct port
netstat -tlnp | grep 5432
```

#### 2. Migration Failed

**Error:** `No CSV files found in Filebrowser`

**Solution:**
```bash
# Verify Filebrowser environment variables
echo $FILEBROWSER_BASE_URL
echo $FILEBROWSER_API_KEY

# Test Filebrowser connection
curl -H "Authorization: Bearer $FILEBROWSER_API_KEY" "$FILEBROWSER_BASE_URL/api/resources"

# Check CSV files exist
curl -H "Authorization: Bearer $FILEBROWSER_API_KEY" "$FILEBROWSER_BASE_URL/api/resources/rollthepay"
```

#### 3. Schema Validation Failed

**Error:** `Duplicate records found`

**Solution:**
```bash
# Check for duplicate records
psql $POSTGRES_URL -c "
SELECT country, COALESCE(state, ''), COALESCE(location, ''), slug_url, COUNT(*)
FROM occupations
GROUP BY country, COALESCE(state, ''), COALESCE(location, ''), slug_url
HAVING COUNT(*) > 1;
"

# Clean up duplicates if needed
psql $POSTGRES_URL -c "
DELETE FROM occupations 
WHERE id NOT IN (
  SELECT MIN(id) 
  FROM occupations 
  GROUP BY country, COALESCE(state, ''), COALESCE(location, ''), slug_url
);
"
```

#### 4. Performance Issues

**Error:** Slow query performance

**Solution:**
```bash
# Refresh materialized views
npm run db:refresh-views

# Reindex tables
npm run db:reindex

# Check index usage
psql $POSTGRES_URL -c "SELECT * FROM pg_stat_user_indexes WHERE schemaname = 'public';"
```

### Performance Monitoring

Monitor these key metrics:

```bash
# Database statistics
psql $POSTGRES_URL -c "SELECT * FROM pg_stat_database WHERE datname = 'rollthepay';"

# Connection statistics
psql $POSTGRES_URL -c "SELECT * FROM pg_stat_activity WHERE datname = 'rollthepay';"

# Index usage
psql $POSTGRES_URL -c "SELECT * FROM pg_stat_user_indexes WHERE schemaname = 'public';"

# Query performance
psql $POSTGRES_URL -c "SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;"
```

## Rollback Plan

If issues occur, you can rollback to Filebrowser:

1. **Restore Filebrowser files:**
   ```bash
   git checkout HEAD~1 -- lib/filebrowser/
   git checkout HEAD~1 -- lib/data/filebrowser-parse.ts
   ```

2. **Restore environment variables:**
   ```bash
   # Add back to .env.local:
   FILEBROWSER_BASE_URL=http://your-filebrowser-server:port
   FILEBROWSER_API_KEY=your_filebrowser_api_key_here
   ```

3. **Restart application:**
   ```bash
   npm run dev
   ```

## Support

For issues or questions:

1. Check the troubleshooting section above
2. Review PostgreSQL and PgBouncer logs
3. Test database connectivity with `npm run db:test-connection`
4. Validate schema with `npm run db:validate`

## Migration Checklist

- [ ] Environment variables configured
- [ ] Database connection tested
- [ ] Schema created successfully
- [ ] CSV data migrated
- [ ] Migration validated
- [ ] Application tested
- [ ] Admin API tested
- [ ] Filebrowser dependencies removed
- [ ] Documentation updated
- [ ] Performance monitoring setup

## Next Steps

After successful migration:

1. **Monitor performance** - Set up monitoring for database metrics
2. **Backup strategy** - Implement regular database backups
3. **User contributions** - Enable user salary updates via Admin API
4. **Scaling** - Consider read replicas for high traffic
5. **Optimization** - Monitor and optimize slow queries

---

**Migration completed successfully!** ðŸŽ‰

Your RollThePay application is now running on PostgreSQL with high-performance connection pooling and user contribution support.
